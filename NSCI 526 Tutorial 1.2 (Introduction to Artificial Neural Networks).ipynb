{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks \n",
    "by Shawn Rhoads, Georgetown University (NSCI 526)\n",
    "<br>(adapted from [Shamdasani](https://dev.to/shamdasani/build-a-flexible-neural-network-with-backpropagation-in-python))\n",
    "***\n",
    "Recall from class, artificial neural networks are typically organized into three main layers: the input layer, the hidden layer, and the output layer. There are several inputs (also called <i>features</i>) that produce output(s) (also called a <i>label(s)</i>)\n",
    "\n",
    "In a feed forward network information always moves one direction without cycles/loops in the network; it never goes backwards ([Wikipedia](https://en.wikipedia.org/wiki/Feedforward_neural_network)):\n",
    "![Feedforward Neural Net](https://thedatamage.com/wp-content/uploads/2018/08/0_0mia7BQKjUAuXeqZ.jpeg)\n",
    "\n",
    "Above, the circles represent \"neurons\" while the lines represent \"synapses\". The role of a synapse is to take the multiply the inputs and weights. You can think of weights as the \"strength\" of the connection between neurons. Weights primarily define the output of a neural network. However, they are highly flexible. After, an activation function is applied to return an output.\n",
    "\n",
    "## Here's a brief overview of how a simple feedforward neural network works:\n",
    "1. Takes inputs as a matrix (2D array of numbers)\n",
    "2. Multiplies the input by a set weights (performs a dot product aka matrix multiplication)\n",
    "3. Applies an activation function\n",
    "4. Returns an output\n",
    "5. Error is calculated by taking the difference from the desired output from the data and the predicted output. This creates our gradient descent, which we can use to alter the weights\n",
    "6. The weights are then altered slightly according to the error.\n",
    "7. To train, this process is repeated 1,000+ times. The more the data is trained upon, the more accurate our outputs will be.\n",
    "\n",
    "> \"They just perform a dot product with the input and weights and apply an activation function. When weights are adjusted via the gradient of loss function, the network adapts to the changes to produce more accurate outputs.\"\n",
    "><br/><br/>(via [Shamdasani](https://dev.to/shamdasani/build-a-flexible-neural-network-with-backpropagation-in-python))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this mean?\n",
    "\n",
    "Let's model a single hidden later with three inputs and one output. In the network, we will be predicting the score of our exam based on the inputs of how many hours we studied and how many hours we slept the day before. Our test score is the output. Here's our sample data of what we'll be training our Neural Network on: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to predict the test score of someone who studied for four hours and slept for eight hours based on their prior performance.\n",
    "\n",
    "Our inputs (`X`) are in hours. Our output (`y`) is a test score from 0-100. Therefore, we need to scale our data by dividing by the maximum value for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hours studied, Hours slept]\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      "[Scores on test]\n",
      "[0.92 0.86 0.89]\n"
     ]
    }
   ],
   "source": [
    "# X = (hours studying, hours sleeping), y = score on test\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
    "y = np.array((92, 86, 89), dtype=float)\n",
    "\n",
    "# scale units\n",
    "X_max = np.amax(X, axis=0)\n",
    "X = X/X_max # maximum of X array\n",
    "y = y/100 # max test score is 100\n",
    "\n",
    "# print\n",
    "print('[Hours studied, Hours slept]')\n",
    "print(X.view())\n",
    "\n",
    "print('\\n[Scores on test]')\n",
    "print(y.view())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synapses perform a dot product of the input and weight. For our first calculation, we will generate random weights between 0 and 1.\n",
    "\n",
    "Our input data, `X`, is a 3x2 matrix. Our output data, `y`, is a 3x1 matrix. Each element in matrix `X` needs to be multiplied by a corresponding weight and then added together with all the other results for each neuron in the hidden layer. \n",
    "\n",
    "First, the products of the random generated weights (.2, .6, .1, .8, .3, .7) on each synapse and the corresponding inputs are summed to arrive as the first values of the hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array(([.2, .6, .1], [.8, .3, .7]), dtype=float)\n",
    "W2 = np.array([.4, .5, .9], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the first input data element (2 hours studying and 9 hours sleeping) would calculate an output in the network:\n",
    "\n",
    "![Example algorithm](./images/feedforward_net_small.png)\n",
    "\n",
    "Here is our first calculation for the hidden layer above:\n",
    "\n",
    "$\\begin{align*} \\mathbf{X_{1}} \\cdot \\mathbf{W1} &= \\begin{bmatrix} x_{11} & x_{12} \\end{bmatrix} \\cdot \\begin{bmatrix} w_{11} & w_{12} & w_{13}  \\\\ w_{21} & w_{22} & w_{23} \\end{bmatrix} \\\\ \\\\ &= \\begin{bmatrix} x_{12}w_{11} + x_{12}w_{21} & x_{12}w_{12} + x_{12}w_{22} & x_{12}w_{13} + x_{12}w_{23} \\end{bmatrix} \\\\ \\\\ &= \\begin{bmatrix} .67 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} .2 & .6 & .1  \\\\ .8 & .3 & .7 \\end{bmatrix} \\\\ \\\\ &= \\begin{bmatrix} (.67*.2 + 1*.8) & (.67*.6 + 1*.3) & (.67*.1 + 1*.7) \\end{bmatrix} \\\\ \\\\ &= \\begin{bmatrix} 0.93 & 0.70 & 0.77 \\end{bmatrix} \\end{align*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the final value for the hidden layer, we need to apply an **activation function**, which will introduce nonlinearity. One advantage of this is that the output is mapped from a range of 0 and 1, making it easier to alter weights in the future.\n",
    "\n",
    "**Enter the sigmoid function:**\n",
    "![Sigmoid](https://res.cloudinary.com/practicaldev/image/fetch/s--lN-oXMal--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/http://www.saedsayad.com/images/ANN_Sigmoid.png)\n",
    "\n",
    "Thus our calculation for the output above:\n",
    "```\n",
    "1 / (1 + np.exp(-0.93)) = 0.72\n",
    "1 / (1 + np.exp(-0.70)) = 0.67\n",
    "1 / (1 + np.exp(-0.77)) = 0.68\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rinse, repeat for output layer:\n",
    "\n",
    "$\\begin{align*} \\beta \\cdot \\mathbf{F} &= \\begin{bmatrix} \\beta_{11} & \\beta_{21} & \\beta_{31} \\end{bmatrix} \\cdot \\begin{bmatrix} f_{11} \\\\ f_{12}  \\\\ f_{13} \\end{bmatrix} \\\\ \\\\ &= \\begin{bmatrix} (\\beta_{11}*f_{11}) + (\\beta_{21}*f_{12}) + (\\beta_{31}*f_{13}) \\end{bmatrix} \\\\ \\\\ &= \\begin{bmatrix} .4 & .5 & .9 \\end{bmatrix} \\cdot \\begin{bmatrix} .72 \\\\ .67 \\\\ .68 \\end{bmatrix} = \\begin{bmatrix} (.4*.72) + (.5*.67) + (.9*.68) \\end{bmatrix} \\\\ \\\\ &= 1.24 \\end{align*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass through activation (sigmoid) function:\n",
    "\n",
    "```\n",
    "1 / (1 + np.exp(-1.24)) = 0.77\n",
    "```\n",
    "\n",
    "Theoretically, our neural network would calculate `.77` as our test score. However, our target was `.92`. Does not perform quite as good as one could hope!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Implementation\n",
    "\n",
    "Now, we are ready to write a forward propagation function. Let's pass in our input, `X`. We can use the variable `z` to simulate the activity between the input and output layers.\n",
    "\n",
    "Remember, we will need to take a dot product of the inputs and weights, apply the activation function, take another dot product of the hidden layer and second set of weights, and lastly apply a final activation function to recieve our output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORWARD PROPAGATION\n",
    "def forward(X,W1,W2):\n",
    "    z = np.dot(X, W1) # dot product of X (input) and first set of 3x2 weights\n",
    "    print(\"z=\")\n",
    "    print(z.view())\n",
    "    print()\n",
    "\n",
    "    z2 = sigmoid(z) # activation function\n",
    "    print(\"z2=\")\n",
    "    print(z2.view())\n",
    "    print()\n",
    "\n",
    "    z3 = np.dot(z2, W2) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
    "    print(\"z3=\")\n",
    "    print(z3.view())\n",
    "    print()\n",
    "\n",
    "    o = sigmoid(z3) # final activation function\n",
    "    print(\"o=\")\n",
    "    print(o.view())\n",
    "    print()\n",
    "\n",
    "    return z,z2,z3,o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(s):\n",
    "    # activation function\n",
    "    return 1/(1+np.exp(-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z=\n",
      "[[0.93333333 0.7        0.76666667]\n",
      " [0.51111111 0.36666667 0.42222222]\n",
      " [0.73333333 0.8        0.56666667]]\n",
      "\n",
      "z2=\n",
      "[[0.71775106 0.66818777 0.68279939]\n",
      " [0.62506691 0.59065328 0.60401489]\n",
      " [0.67553632 0.68997448 0.63799367]]\n",
      "\n",
      "z3=\n",
      "[1.23571376 1.0889668  1.18939607]\n",
      "\n",
      "o=\n",
      "[0.77481705 0.74818711 0.76663304]\n",
      "\n",
      "Predicted Output: \n",
      "[0.77481705 0.74818711 0.76663304]\n",
      "Actual Output: \n",
      "[0.92 0.86 0.89]\n"
     ]
    }
   ],
   "source": [
    "z,z2,z3,o = forward(X,W1,W2)\n",
    "\n",
    "print(\"Predicted Output: \\n\" + str(o))\n",
    "print(\"Actual Output: \\n\" + str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how terrible this performs! With our simple feedforward network, we aren't able to predict our test scores very well. Why? Our network needs to learn!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Backpropagation\n",
    "\n",
    "Since we initialize with a random set of weights, we need to alter them to make our inputs equal to the corresponding outputs from our data set. This is done through **backpropagation**, which works by using a **loss function** to calculate how far the network was from the target output. \n",
    "\n",
    "Like the activation function, there is no one-size-fits-all loss function. Two common loss functions include: \n",
    "\n",
    "__mean absolute error (L1 Loss):__ measured as the average of sum of absolute differences between predictions and actual observations; more robust to outliers since it does not make use of square\n",
    "\n",
    "$$\n",
    "MAE = \\frac {\\sum_{i=1}^n |y_i - \\hat{y_i}|}{n}\n",
    "$$\n",
    "\n",
    "__mean square error (L2 Loss):__ the average of squared difference between predictions and actual observations; due to squaring, predictions that are far away from actual values are penalized heavily in comparison to less deviated predictions\n",
    "\n",
    "$$\n",
    "MSE = \\frac {\\sum_{i=1}^n (y_i - \\hat{y_i})^2}{n}\n",
    "$$\n",
    "\n",
    "Using our example: `o` is our predicted output, and `y` is our actual output. Our goal is to get our loss function as close as we can to `0`, meaning we will need to have close to no loss at all. \n",
    "\n",
    "Training = minimizing the loss. \n",
    "\n",
    "$$\n",
    "Loss = \\frac {\\sum (o - y)^2}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Gradient Descent\n",
    "To figure out which direction to alter our weights, we need to find the rate of change of our loss with respect to our weights (i.e., we need to use the derivative of the loss function to understand how the weights affect the input):\n",
    "\n",
    "![gradient descent](https://raw.githubusercontent.com/bfortuner/ml-cheatsheet/master/docs/images/gradient_descent_demystified.png)\n",
    "\n",
    "*via https://github.com/bfortuner/ml-cheatsheet/blob/master/docs/gradient_descent.rst*\n",
    "\n",
    "### Here's how we will calculate the incremental change to our weights:\n",
    "1. Find the margin of error of the output layer `o` by taking the difference of the predicted output and the actual output `y`.\n",
    "2. Apply the derivative of our sigmoid activation function to the output layer error. We call this result the **delta output sum**.\n",
    "3. Use the delta output sum of the output layer error to figure out how much our `z2` (hidden) layer contributed to the output error by performing a dot product with our second weight matrix. We can call this the z^2 error.\n",
    "4. Calculate the delta output sum for the `z2` layer by applying the derivative of our sigmoid activation function (just like step 2).\n",
    "5. Adjust the weights for the first layer by performing a dot product of the input layer with the **hidden delta output sum**. For the second weight, perform a dot product of the hidden(`z2`) layer and the **output (`o`) delta output sum**.\n",
    "\n",
    "Calculating the delta output sum and then applying the derivative of the sigmoid function are very important to backpropagation. The derivative of the sigmoid, also known as **sigmoid prime**, will give us the rate of change, or slope, of the activation function at output sum.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop Implementation\n",
    "Let's continue to code our `Neural_Network` class by adding a `sigmoidPrime` (derivative of sigmoid) function and a `backward` propagation function that performs the four steps above. \n",
    "\n",
    "Then, we can define our output through initiating foward propagation and intiate the backward function by calling it in a `train` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKPROPAGATION with Mean Square Error Minimization\n",
    "def backward(X,W1,W2,y,z,z2,z3,o):\n",
    "    # backward propgate through the network\n",
    "    o_error = y - o # error in output\n",
    "    print(\"Error in output (o_error):\")\n",
    "    print(o_error.view())\n",
    "    print()\n",
    "\n",
    "    o_delta = o_error*sigmoidPrime(o)\n",
    "    print(\"Applied gradient of sigmoid to error (o_delta):\")\n",
    "    print(o_delta.view())\n",
    "    print()\n",
    "\n",
    "    z2_error = o_delta.dot(W2.T) \n",
    "    print(\"How much our hidden layer weights contributed to output error (z2 error):\")\n",
    "    print(z2_error.view())\n",
    "    print()\n",
    "\n",
    "    z2_delta = z2_error*sigmoidPrime(z2)\n",
    "    print(\"Applied gradient of sigmoid to z2 error (z2_delta):\")\n",
    "    print(z2_error.view())\n",
    "    print()\n",
    "\n",
    "    W1_b = W1 + X.T.dot(z2_delta)\n",
    "    print(\"Adjusting first set (input --> hidden) weights:\")\n",
    "    print(\"W1=\")\n",
    "    print(W1_b.view())\n",
    "    print()\n",
    "\n",
    "    W2_b = W2 + z2.T.dot(o_delta)\n",
    "    print(\"Adjusting second set (hidden --> output) weights:\")\n",
    "    print(\"W2=\")\n",
    "    print(W2_b.view())\n",
    "    print()\n",
    "    \n",
    "    return W1_b, W2_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidPrime(s):\n",
    "    #derivative of sigmoid (e.g., gradient)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try running one iteration of backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORWARD PROPOGATION #1\n",
      "\n",
      "z=\n",
      "[[0.93333333 0.7        0.76666667]\n",
      " [0.51111111 0.36666667 0.42222222]\n",
      " [0.73333333 0.8        0.56666667]]\n",
      "\n",
      "z2=\n",
      "[[0.71775106 0.66818777 0.68279939]\n",
      " [0.62506691 0.59065328 0.60401489]\n",
      " [0.67553632 0.68997448 0.63799367]]\n",
      "\n",
      "z3=\n",
      "[1.23571376 1.0889668  1.18939607]\n",
      "\n",
      "o=\n",
      "[0.77481705 0.74818711 0.76663304]\n",
      "\n",
      "----------------------\n",
      "\n",
      "BACK PROPOGATION #1\n",
      "\n",
      "Error in output (o_error):\n",
      "[0.14518295 0.11181289 0.12336696]\n",
      "\n",
      "Applied gradient of sigmoid to error (o_delta):\n",
      "[0.02533088 0.0210659  0.02207119]\n",
      "\n",
      "How much our hidden layer weights contributed to output error (z2 error):\n",
      "0.040529375374716514\n",
      "\n",
      "Applied gradient of sigmoid to z2 error (z2_delta):\n",
      "0.040529375374716514\n",
      "\n",
      "Adjusting first set (input --> hidden) weights:\n",
      "W1=\n",
      "[[0.21752339 0.61792664 0.11844388]\n",
      " [0.81940985 0.32020968 0.72040389]]\n",
      "\n",
      "Adjusting second set (hidden --> output) weights:\n",
      "W2=\n",
      "[0.44625876 0.54459699 0.94410131]\n",
      "\n",
      "----------------------\n",
      "\n",
      "FORWARD PROPOGATION #2\n",
      "\n",
      "z=\n",
      "[[0.96442544 0.73216077 0.79936647]\n",
      " [0.52773549 0.38386981 0.43970567]\n",
      " [0.76379662 0.83139976 0.59871313]]\n",
      "\n",
      "z2=\n",
      "[[0.72400698 0.67527926 0.68983895]\n",
      " [0.6289548  0.59480611 0.6081889 ]\n",
      " [0.68217745 0.69665082 0.64536184]]\n",
      "\n",
      "z3=\n",
      "[1.34212735 1.17879814 1.29310855]\n",
      "\n",
      "o=\n",
      "[0.79283957 0.76473164 0.78467288]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"FORWARD PROPOGATION #1\\n\")\n",
    "z,z2,z3,o = forward(X,W1,W2)\n",
    "print(\"----------------------\\n\")\n",
    "print(\"BACK PROPOGATION #1\\n\")\n",
    "W1_b, W2_b = backward(X,W1,W2,y,z,z2,z3,o)\n",
    "print(\"----------------------\\n\")\n",
    "print(\"FORWARD PROPOGATION #2\\n\")\n",
    "z,z2,z3,o = forward(X,W1_b,W2_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the outputs are much better after implementing backprop twice.. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train our network!\n",
    "\n",
    "We will define a python `class` and insert our functions from above. We will add an `init` function where we'll specify our parameters such as the input, hidden, and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hours studied, Hours slept]\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      "[Scores on test]\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n"
     ]
    }
   ],
   "source": [
    "# X = (hours studying, hours sleeping), y = score on test\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
    "y = np.array(([92], [86], [89]), dtype=float)\n",
    "\n",
    "# scale units\n",
    "X_max = np.amax(X, axis=0)\n",
    "X = X/X_max # maximum of X array\n",
    "y = y/100 # max test score is 100\n",
    "\n",
    "# print\n",
    "print('[Hours studied, Hours slept]')\n",
    "print(X.view())\n",
    "\n",
    "print('\\n[Scores on test]')\n",
    "print(y.view())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #parameters\n",
    "        self.inputNum = 2 # two inputs\n",
    "        self.outputNum = 1 # 1 ouput\n",
    "        self.hiddenNum = 3 # 3 nodes in our hidden layer\n",
    "\n",
    "        #weights\n",
    "        np.random.seed(2019) #set random seed for reproducibility\n",
    "        self.W1 = np.random.randn(self.inputNum, self.hiddenNum) # (3x2) weight matrix from input to hidden layer\n",
    "        self.W2 = np.random.randn(self.hiddenNum, self.outputNum) # (3x1) weight matrix from hidden to output layer\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #forward propagation through our network\n",
    "        self.z = np.dot(X, self.W1) # dot product of X (input) and first set of 3x2 weights\n",
    "        self.z2 = self.sigmoid(self.z) # activation function\n",
    "        self.z3 = np.dot(self.z2, self.W2) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
    "        o = self.sigmoid(self.z3) # final activation function\n",
    "        return o \n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        # activation function\n",
    "        return 1/(1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        #derivative of sigmoid\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def backward(self, X, y, o):\n",
    "        # backward propgate through the network\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.W2.T) # z2 error: how much our hidden layer weights contributed to output error\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error\n",
    "\n",
    "        self.W1 += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
    "        self.W2 += self.z2.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
    "        \n",
    "    def train (self, X, y):\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "        \n",
    "    def test(self, X):\n",
    "        # test using new data\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the network, all we have to do is to run the `train` function. \n",
    "\n",
    "We will want to do this multiple (e.g., hundreds) of times. So, we'll use a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interation: #0\n",
      "Input: \n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.59829797]\n",
      " [0.58952764]\n",
      " [0.58856997]]\n",
      "Mean square error: \n",
      "0.089169184931786\n",
      "\n",
      "\n",
      "Interation: #50\n",
      "Input: \n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.89614134]\n",
      " [0.86612614]\n",
      " [0.8936332 ]]\n",
      "Mean square error: \n",
      "0.00020665518467188445\n",
      "\n",
      "\n",
      "Interation: #100\n",
      "Input: \n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.90013936]\n",
      " [0.87028857]\n",
      " [0.89757326]]\n",
      "Mean square error: \n",
      "0.00018588463004548227\n",
      "\n",
      "\n",
      "Interation: #150\n",
      "Input: \n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.90052777]\n",
      " [0.87063133]\n",
      " [0.89779311]]\n",
      "Mean square error: \n",
      "0.0001843085450576197\n",
      "\n",
      "\n",
      "Interation: #199\n",
      "Input: \n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.90062752]\n",
      " [0.87067013]\n",
      " [0.89772119]]\n",
      "Mean square error: \n",
      "0.00018292052710009487\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "mse = np.array([]) # let's track how the mean sum squared error goes down as the network learns \n",
    "\n",
    "n_iter = 200\n",
    "for i in range(n_iter): # trains the network n_iter times\n",
    "    mse = np.append(mse,np.mean(np.square(y - NN.forward(X)))) #store error\n",
    "    # let's print the output every (n_iter/4) iteration and last:\n",
    "    if i % (n_iter/4) == 0 or i == (n_iter-1):\n",
    "        print(\"Interation: #%i\" % (i))\n",
    "        print(\"Input: \\n\" + str(X) )\n",
    "        print(\"Actual Output: \\n\" + str(y)) \n",
    "        print(\"Predicted Output: \\n\" + str(NN.forward(X)) )\n",
    "        print(\"Mean square error: \\n\" + str(mse[i])) # mean sum squared error\n",
    "        print(\"\\n\")\n",
    "    NN.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the mean sum squared error goes down with each iteration. That's the network learning via gradient descent! \n",
    "\n",
    "We can also plot the gradient descent below (notice how quickly it converges):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(n_iter), mse)\n",
    "\n",
    "ax.set(xlabel='iteration', \n",
    "       ylabel='MSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our network with new data (validation):\n",
    "How well does our classifier perform on unseen data? To test how well it performs on new data, let's pretend we split our hours studying/sleeping data into training and testing samples. The example above uses few training samples (N=3) to train our network. Let's use a \"testing sample\" to validate our neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted scores=\n",
      "[0.9, 0.86, 0.88, 0.91, 0.89, 0.9]\n",
      "actual scores=\n",
      "[0.9, 0.88, 0.91, 0.95, 0.9, 0.92]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(([2, 8], [1, 4], [2, 4], [3, 8], [1,8], [2,9]), dtype=float)\n",
    "X_test = X_test/X_max\n",
    "y_actual = np.array(([.9], [.88], [.91], [.95], [.90], [.92]), dtype=float)\n",
    "\n",
    "# Test \n",
    "y_pred = NN.test(X_test)\n",
    "\n",
    "print(\"predicted scores=\")\n",
    "print([round(y_pred[i,0],2) for i in range(len(y_pred))])\n",
    "\n",
    "print(\"actual scores=\")\n",
    "print([y_actual[j,0] for j in range(len(y_actual))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 79.3%\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "accuracy = pearsonr(y_pred,y_actual)\n",
    "print(str('accuracy = %03.1f%%' % (accuracy[0][0]*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_pred, y_actual)\n",
    "ax.set(xlabel='actual score',\n",
    "ylabel='predicted score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
